// Copyright (c) 2009-2019 The Regents of the University of Michigan
// This file is part of the HOOMD-blue project, released under the BSD 3-Clause License.


/*!
\page page_dev_info Developer design notes

Note: Some of these notes have not been modified since they were originally written
and may be out of date with current practices.

Table of contents:
- \subpage page_overview
	- \ref sec_goals
	- \ref sec_hardware
	- \ref sec_code_impl_overview
- \subpage page_coding_practice
	- \ref sec_documenting_code
	- \ref sec_defensive_prog
	- \ref sec_unit_testing
	- \ref sec_profiling
	- \ref sec_floats
	- \ref sec_multithreaded
	- \ref sec_libraries
	- \ref sec_sse_instrns
- \subpage page_particle_data
	- \ref sec_particle_data_rationale
	- \ref sec_particle_data_requirements
- \subpage page_compute
	- \ref sec_compute_rationale
	- \ref sec_compute_requirements
- \subpage page_updater
	- \ref sec_updater_rationale
	- \ref sec_updater_requirements
- \subpage page_file_format
	- \ref sec_file_format_rationale
	- \ref sec_file_format_requirements
- \subpage page_system_class_design
	- \ref sec_system_class_rationale
	- \ref sec_system_class_requirements
- \subpage page_hoomd_script_design
- \subpage page_gpu_impl
- \subpage page_rigid
*/

/*!
\page page_overview Overview

\section sec_goals Goals

HOOMD stands for highly-optimized object oriented molecular dynamics. The focuses are twofold.
1) Build the fastest possible <em>general purpose</em> molecular dynamics code for a single
workstation. Existing software
packages are already extremely well optimized for huge clusters and supercomputers, where millions
and even billions of particles can be simulated. HOOMD will not even attempt to meet such a a level of
performance. It is instead targeted at smaller, more modest systems of 100,000 particles and smaller.
Every potential optimization aimed at the single workstation level will be employed. 2) Keeping the
performance targets in mind, every effort is made to create an abstract object oriented interface
to allow for easy expansion with new force fields, analysis tools, etc...

\section sec_hardware Targeted hardware

CPUs are becoming more and more multi-core. Currently one can buy 8-core systems (two quad-core
processors) at reasonable prices. HOOMD will take advantage of these systems and make use
of every core in such a workstation. Additionally, graphics cards have become extremely
powerful general purpose computational tools. HOOMD will have a mode, optionally compiled
in to make use of GPUs supporting NVIDIA's CUDA. Fitting in with the object oriented design,
every performance critical part of the simulation will have dual implementations: one for
multi-core CPUs and one for GPUs. Through the beauty of data-hiding object oriented design,
one portion of the simulation need not know if anther is being run on the GPU or CPU. So,
different steps in the code can be swapped out at will, which is not so useful from a performance
point of view, but it is very helpful in debugging. Additionally, some tasks, such as
writing dump files, simply must be performed on the CPU. The object-oriented design
will seamlessly handle all required data-transfer back and forth GPU <--> CPU to make
this happen.

In accordance with these goals, HOOMD will be optimized for commonly available hardware.
Specifically, Intel/AMD mainstream processors will be supported running under Linux, Windows XP,
and Mac OS X. As such, the code must be written in a general enough fashion that it can run on
these systems with little or no conditional compilation. OS specific calls should be avoided
if at all possible to allow for other operating systems to work with minimal code modifications.

Hardware specific optimizations (i.e. SSE) can and should be employed where appropriate.
However, it should only be available via conditional compilation and a vanilla C++ implementation
should also be provided to support other hardware systems.

\section sec_code_impl_overview Code implementation overview

To meet the multiple OS design requirement, CMake (see www.cmake.org) will be employed. CMake
takes a simple text file format input that describes what source files should be compiled into
what executables and libraries. It can then generate makefiles for linux, XCode projects on
Mac OS X and Visual Studio projects on Windows. So developers that prefer a particular architecture
can work on the code in a natural environment.

-# System: At the highest level of abstraction is the System. It abstracts a system of particles
	in a box and performs updates of those particles in time, saves/loads restart files,
	writes a log output etc...
-# ParticleData: The most fundamental data structure which the System owns is the particle data.
	Data stored will include particle id tag, type, position, velocity, and acceleration.
	Other traits may be added, but will not be considered fundamental.
-# Compute: A Compute is a concept that will take the particle data and compute something useful,
	whether it be a single scalar for the entire system or a per particle quantity. The
	particle data is NOT to be modified by the compute. One compute may reference other computes.
	- Note: Credit for the idea of the compute concept goes to LAMMPS developers, but HOOMD takes
		that idea and generalizes it to a whole new level.
-# Updater: An Updater takes the particle data and updates it in some way. The simplest example of this
	is the updating of particle positions by the velocity verlet integration. Though, any
	other type of operation that takes the particle properties and modifies them on a per
	time step basis is an Updater.
-# Analyzer: An Analyzer takes the data at a given time step, performs some analysis and outputs results.
	Most analyzers will likely output data every M steps. An Analyzer may reference one or more computes,
	and is not allowed to modify the particle data.
-# ParticleDataInitializer: An initializer takes some kind of input (data file, parameters, etc...) and
	generates the initial condition for the System's particles.

There is a fine line between an analyzer and a compute. Computes are designed to be calculated when
needed and their results used in to support Updaters. Analyzers, instead, are only to produce
output based on the particle data, but their results cannot result in a change of the system in time.
The typical implementation of an Analyzer would write analyzed data to a file.

So where are the forces, the neighborlists, etc... that are always a part of any MD code? Of course,
these are just implementations of the concepts listed above. Calculating forces are the result of
a Compute. They use the neighborlist which is the result of another Compute. NVT integration is
performed in an Updater which uses the results of the force compute. Everything flows together in
a natural way while still keeping every class as independent as possible from the others. The only
common link between <b>ALL</b> classes is the ParticleData.
<hr>
\ref page_coding_practice "Next page"
*/

/*! \page page_coding_practice Coding practices

\section sec_documenting_code Documentation

All code should be <b>thoroughly</b> documented via doxygen (the document you are reading now) and
internal comments to make life as easy as possible on developers wishing to modify the source code
or add new features. See http://www.stack.nl/~dimitri/doxygen/commands.html for information on how
to use doxygen comments.

\section sec_defensive_prog Defensive Programming

Error checking should be a top priority. Asserts will be used liberally to verify that objects
are in the proper state upon entrance to a function call, and even the validity of values calculated
inside inner loops. These can safely be used even in performance critical parts of the code, as asserts
are only compiled in debug mode. Asserts should be used to check for mainly for programming errors.
Any errors that might result at runtime due to a user's action should still be checked in release mode.
Obviously, not every possible such error can be checked without sacrificing performance, so runtime
error checks in an optimized build should only be performed during initialization, or once per time step
at the most.

A simple approach to printing these messages is employed in HOOMD. Warnings are simply printed to
stdout. The preferred format to call attention to the warning is: "***Warning! warning text"
Errors are printed to stderr with the following format (example code shown for C++ and python:
\code
cerr << endl << "***Error! error message text" << endl << endl;
print >> sys.stderr, "\n***Error! error message text\n";
\endcode
The reason for the blank line before and after the message is so that the message is visible is the sea
of test that comes from the python backtrace when the error is printed out.
Errors that prevent the continued execution of HOOMD throw an exception: RuntimeError in python
and std::runtime_error in C++.

\section sec_unit_testing Unit testing

Along with defensive programming, EACH AND EVERY class should be unit tested independent from
the rest of the code. The boost unit testing framework will be used for this. No class should
be used in production research code without passing very extensive tests. For instance, a compute
that calculates forces could be fed a very small 3 particle system and the results checked against
a by-hand calculation. Or an optimized version of a force computing code could be run on a large
system and the results compared to a simpler version of the compute operating on the same
system (which was independently verified, of course).

\section sec_profiling Profiling

A simple Profiler class will be available to Computes, Updaters, and Systems. This profiler
should be used in a coarse manner to give simple timing statistics of each part of the algorithm.
The profiling code also provides a mechanism by which to record the number of FLOPS executed
and memory transferred so that a guesstimate can be made of how close portions of the code
are to optimal performance.

Time critical portions of the code should be profiled in AMD CodeAnalyst to discover the bottlenecks.

\section sec_floats Floating point numbers

All code will be written with a typedef'd Scalar type. This will be either float or
double at compile time. The reasons for this are many. 1) The GPU is single precision only,
so we need a way to get the rest of the code in single precision. 2) It may be interesting to
compare performance/accuracy between float and double in the CPU only code 3) This
will NOT be done as a template to avoid code complexity.

\section sec_multithreaded Multithreaded design

The design requirement to target multi-core systems necessitates the use of multi-threaded code
Some simple performance tests were made an on 8-core machine to make an informed choice
for the design. The thread-model adopted aims itself at code simplicity without sacrificing
performance.

The System class will be single threaded and play the role of a <em>boss</em> thread. As it calls
updaters, who call their attached computes: threading will be accomplished by worker threads. That is,
the interface presented by an Updater or a Compute is a single-threaded interface. However,
each Compute will have launched it's own set of worker threads. When the single-threaded
interface function is called to begin computing results, these worker threads will be woken up,
perform their computation while the single-threaded interface call waits for them to finish.
In this manner, the caller doesn't know or care if the callee is using multiple threads,
which keeps the code very abstract and very easy to understand and modify.

\section sec_libraries External libraries used

Threading: <b>boost.threads</b> <br>
Compression: <b>boost iostream filters</b> <br>
Python integration: <b>boost.python</b> <br>
Serialization: <b>boost.serialization</b> <br>
Shared pointers: <b>boost.shared_ptr</b> <br>
GPU access: <b>NVIDIA CUDA</b> <br>
XML Parsing: http://www.applied-mathematics.net/tools/xmlParser.html

If you didn't get the hint, boost (www.boost.org) will be used extensively. It is a very nice c++
library that provides a lot of power in a very nice and very, very abstract syntax. A minimum
of external libraries should be required, so users don't need to install too many in order
to run the software. Fortunately, most recent linux distributions in the last few years have
boost in their default install, even if it is an old version of boost.

As a general practice, any time an external library is needed, boost should be checked first.
If boost cannot do what is needed, then other libraries can be evaluated. To meet the goals
of HOOMD, any such library chosen must have cross-platform support and preferably be installed
by default in standard linux distributions.

The pain and time needed to install boost and all the tools needed to build HOOMD on windows
is significant, so a pre-compiled binary should be provided there.

\section sec_sse_instrns SSE

SSE instructions can, and should, be used where appropriate. However, as some of the targeted
machines do not have SSE instructions, code that performs the same calculation without SSE
instructions should be included. SSE code should be conditionally compiled in with the
define __SSE__ or __SSE2__ as needed. Additionally, SSE instructions should only be used
if USE_SSE is defined. Some problems arose testing the code with the Pathscale compiler
when trying to compile SSE code. Allowing the user to disable SSE entirely would allow
HOOMD to be built with Pathscale without code modifications.

<hr>
\ref page_particle_data "Next page"
*/

/*!
\page page_particle_data Particle data class design

\section sec_particle_data_rationale Rationale

The data structure that stores the particles is the fundamental building block upon
which the rest of the code must rest. Thus, it must be done right. Low level access
must be allowed to the data, but such access must be abstract enough so that the user
of the data doesn't need to know if the data was most recently updated on the CPU
or GPU. Sorting algorithms must be employed to obtain optimal performance for large
numbers of particles. The particle data structure should therefore be able to handle
reordering of the particles in memory without loosing track of who is who. Lastly, the particle
data structure will also contain the simulation box setup, as this seems the best place to put it.

\section sec_particle_data_requirements Requirements

- Data Storage:
	- Stores positions (x,y,z), velocities (vx,vy,vz), and accelerations (ax,ay,az)
		for each particle all at the current time step
	- Stores each particle's type and id tag
	- To help a little bit with caching, all of these arrays should be contiguous in
		memory. This will also make it easier to make a copy of the particle data
		structure.
- Serializable (<b>TODO</b>)
- Non-copyable
- Provide an acquire/release mechanism for low level access to the data. Read only
	and read/write accesses should be separately handled. Data can be acquired
	either on the CPU or GPU
- Support both single and double precision through a define
- Stores the simulation box (xlo, xhi, ylo, ...).  All dimensions are assumed periodic.
	This is not a serious limitation because dimensions can always be made larger and
	walls added for non-periodic simulations.
	- xlo is required to be -xhi/2, and similarly for y and z for performance reasons on the GPU
- The particle data structure is assumed to have a constant number of particles over its lifetime.
- Provide a print statistics function (<b>TODO</b>)

See ParticleData for the implementation of this class.
<hr>
\ref page_compute "Next page"
*/

/*!
\page page_compute Compute class design

\section sec_compute_rationale Rationale

Much of the rationale for this class is documented in the \ref page_overview section. Here, some
particular choices for implementing the concept are rationalized.

Since computes are allowed to reference other computes, and analyzers might just call a compute
too, it is possible that one compute shared among others will be asked to compute its results
more than once in a given time step. This would be a waste of resources, and should be avoided.
It will be avoided by providing the current time step as an argument to the member function
that performs the computation. This way, the compute can check if the time step specified in the
current call is the same as the last call and skip the computation. As an added benefit, time
dependent quantities can become part of a compute as it will always know what the current
time step is.

Statistics on memory usage, number and character of computations performed, etc ... may be useful
for the user to see. <b>In no way</b> can this statistics tracking pose an impact to the performance
of the compute when it is running normally. Of course, the actual call to printStats() can have any
amount of overhead it wishes. If the user wants detailed statistics over many time steps, they
can write an Analyzer.

A Compute is not a functor. It is an object that computes data for a particular ParticleData.
It can and should store tables that only need to be updated rarely to allow faster computations.
As such, a single Compute is tied to a single ParticleData on instantiation. The reason for this
is that any per particle tables can be initialized here and never need to be resized, making
code simple and easy to read and modify. <b>Note:</b> This requirement may be relaxed at
some time in the future, probably by allowing a ParticleData to reinitialize and then send
a reinit() signal to all computes. For now, though, the initialization on instantiation
requirement remains.

\section sec_compute_requirements Requirements

<b>Base class requirements:</b>
- Provide a virtual compute() method that calculates the result
	- This method should take the time step as a parameter.
	- Some basic helper methods should be provided so that derived classes can easily tell if they have already been updated this step.
- Serializable (<b>TODO</b>)
- Stores a pointer to the particle data, initialized on instantiation
- Provide a virtual print statistics function (and clear statistics, so the user can do so if they wish)
- Store an optional pointer to a Profiler for use in profiling the compute
- Non-copyable

<b>Derived class requirements:</b>
- Serializable
- Implements the compute and abstract access methods
- Defines methods by which to access the compute's results
	- Care should be taken in designing this interface so the design is usable for any future child class that is run on the GPU

<hr>
\ref page_updater "Next page"
*/

/*!
\page page_updater Updater class design

\section sec_updater_rationale Rationale

Much of the rationale for this class is documented in the \ref page_overview section. Here, some
particular choices for implementing the concept are rationalized.

Much of the design decisions made for the Updater are the same as for the Compute, and the
rationale is not repeated here. See \ref page_compute for more information.

One difference is that Updaters should <b>NOT</b> be called more than once per time step.
There should be no reason to, and updaters can assume that they are not. This is for one
very simple reason, an updater by definition, advances the simulation to the <b>next</b>
time step. More details on the intricacies of how this applies to multiple updaters
in the documentation for System.

\section sec_updater_requirements Requirements

<b>Base class requirements:</b>
- Provide a virtual "update" method that changes the state of the particles
	- This method should take the time step as a parameter.
- Serializable
- Stores a pointer to the particle data (initialized on instantiation)
- Store an optional pointer to a Profiler for use in profiling the compute
- Non-copyable

These requirements are implemented by the Updater class.

<b>Derived class requirements:</b>
- Serializable
- Implements the update method

<hr>
\ref page_file_format "Next page"
*/

/*!
\page page_file_format File format design

\section sec_file_format_rationale Rationale

A well defined file format is required to provide the initial configurations of particles
for HOOMD to run. Particle positions and velocities are also typically saved to dump
files every so many time steps. A common use case is to take an existing dump file and
use it as the initial configuration for a new run, so the two files should share a common
format. One complication introduce by this use case is that it is usually a waste of
space to save bonds to every dump file. Thus, reusing that dump file as an input
would result in no bonds! The solution to this use case is not implemented in the
HOOMDInitializer class to avoid complications. Instead, an external python script
to merge bonds from one file into another will be provided.

Depending on the needs of the user, a variety of data can be included into the saved
dump file, including particle positions, bonds, velocities, and forces. The file format needs
to be expandable so that in the future, charges and other quantities can be easily
added without disrupting the existing file format.

Files written during the simulation should be one file for each time step written. In
case the simulation crashes or stops prematurely, we don't want a partially buffered file
to be not completely written to the disk. Furthermore, the time step should be appended
to the file name with 0 padding to 10 digits so that "alphabetically" sorted file names
will be read in the order of increasing time steps.

Large particle systems need to be supported with a minimum of file size. Only as much
data as needed should be saved. However, a string of raw numbers is not very descriptive.
Only by knowing the parameters that produced the output can one know what the numbers mean.
So, the file format must include metadata the describes just that without requiring a
reference to the parameters used to create the output. XML is widely used and easy to
understand, both by humans and machines, so it will be used for the metadata.

As an additional requirement for reducing file size: files ending in a ".gz" extension
should be transparently decompressed. Similarly, the writer should support writing to gzipped
files (as on option). Boost gzip streams for this: http://www.boost.org/libs/iostreams/doc/classes/gzip.html
will be used for this.

Note: The boost example code only shows how to use a filtering stream<b>buf</b>. A
filtering_stream can be used in an identical way:

\verbatim
ifstream file("testdata.gz");
filtering_stream<input> in;
in.push(gzip_decompressor());
in.push(file);

in >> data; // or anything you normally can do with an ifstream
\endverbatim

XML Parsers were evaluated, and I found a simple one that has a nice interface
and not a lot of overhead: http://www.applied-mathematics.net/tools/xmlParser.html

\section sec_file_format_requirements Requirements
<b>File format:</b>
-# The time step must be specified in the file
-# Box dimensions must be included (with units)
-# The number of particles in the file must be specified
-# The number of particle types must also be specified
-# Particle positions (with units) can optionally be included
-# Particle velocities (with units) can optionally be included
-# Particle types can optionally be included
-# Lists of numbers (positions, velocities, types) will be given as text readable numbers
	in the order of the particle tags. Individual numbers will be separated by whitespace.
	The particular type of whitespace should be irrelevant.
-# File size is important, numbers written should look like 12.123456, that is: only 6 numbers
	after the decimal.
-# The order of xml elements in the file should not matter.
-# Bonds should also be specified

Here is a proposed xml file that meets these requirements. Replace xn yn zn with floating
point numbers that are the coordinates of the particle with tag n, and the same goes for vxn, vyn, vzn.
typen is an integer (from 0 to NTypes-1) that specifies the type of the particle with tag n.

\verbatim
<?xml version="1.0" encoding="UTF-8"?>
<hoomd_xml>
<configuration time_step="0" N="5" NTypes="2">
<box units="sigma" Lx="13.456789" Ly="12.123456" Lz="45.765432" />
<position units="sigma">
x1 y1 z1
x2 y2 z2
x3 y3 z3
x4 y4 z4
x5 y5 z5
</position>
<velocity units="sigma/tau">
vx1 vy1 vz1
vx2 vy2 vz2
vx3 vy3 vz3
vx4 vy4 vz4
vx5 vy5 vz5
</velocity>
<type>
type1
type2
type3
type4
type5
</type>
</configuration>
</hoomd_xml>
\endverbatim

Don't forget the requirement that the order of elements doesn't matter. The box element
should be able to be placed just before the /configuration without bothering the reader.
This can be accomplished with the "Find element" methods in the xml code linked to above.

Requirements for the reader:
-# Can read the file format defined above
-# Implemented as a ParticleDataInitializer
-# Should write status messages to cout what elements are being read from the file
		so the user is sure things are coming from the right place.
-# Similarly, it should print status messages to cout listing ignored elements.

The reader is implemented in HOOMDInitializer

Requirements for the writer
-# Can write the file format defined above
-# Implemented as an Analyzer
-# Given a base file name, say "dump/atoms.dump" for example: The writer should
	write one file per time step, appending the time step with zero padding.
	The example for time step 15000 would be "dump/atoms.dump.0000015000".
-# As an option, the creator of the writer can request that dump files can be saved in
	a gzipped format. In this case, an additional ".gz" should be appended after the
	time step.
-# The box, position, velocity, and type elements need to be optionally included/excluded
	from the file.
	- Note: a reasonable default is to include box, position, and type
-# Forces can also be written to the file (TO BE IMPLEMENTED LATER)

The writer is implemented in HOOMDDumpWriter

An external script will be written to combine elements from separate files into one
in order to implement the use case mentioned above.

<hr>
\ref page_dev_info "Back to Table of Contents"

\page page_system_class_design System Class Design

\section sec_system_class_rationale Rationale

The System class ties everything together into one conglomerate. It defines
the meaning of time step and is responsible for stepping the system
through time. This is done for several reasons. 1) It has to be done
somewhere :) 2) Convenience: add things to System and then call
run to step forward so many time steps. and 3) everything is referenced
to a central object so restart files can be implemented by
just serializing System.

The integration with python (http://trac2.assembla.com/hoomd/ticket/40) is
likely going to require that the tracked computes, updaters, etc...
are named.

The System class will store an arbitrary number of Analyzer classes,
Updater classes, and Compute classes. Computes are only stored for
reference when saving/loading restart files.

It will provide a run method that runs for a specified number of time steps.
During the run of each step
 -# Analyzers are executed every \b period time steps
 -# Updaters are executed every \b period time steps
 -# The Integrator is executed advancing the simulation to tstep+1

Analyzers and updaters will be executed in the same order in which they are
added to the System. The period for executing analyzers can be configured
on a per analyzer basis. Updaters can also be set to only update
every so many steps.

Status information will be printed while the simulation is running. Every
N seconds a line will be printed noting the total run time so far, the number
of time steps completed out of the total and an estimate on the run completion time.
Using LAMMPS, I've always thought this kind of output would be useful.

Since the profiler brings everything together, it makes sense as a place to
set/clear the profiler too.

\section sec_system_class_requirements Requirements

 - Constructed from a ParticleData reference, which it holds on to
 - Tracks updaters/analyzers added to it and the order, allowing for addition, removal, and period changing.
 - One Integrator can be set
 - Provides a run method to run a certain number of time steps
 	- run will write out status information
 - Serializable (\b TODO)
 - Set profiler on all stored analyzer, updaters, and computes

<hr>
\ref page_dev_info "Back to Table of Contents"

\page page_hoomd_script_design Python module hoomd_script design

\section sec_hoomd_script_rationale Rationale

The power of LAMMPS (lammps.sandia.gov) comes from its high configurability through its
scripting system. HOOMD needs to have a scripting system with similar or more power.
To facilitate rapid application development and to provide the most configurable
system possible to the end user, HOOMD will use python (www.python.org) as the
script environment. This opens the door for users to perform complicated calculations
and variable assignments in their scripts, or even easily embed HOOMD as an MD engine
in another application. All this comes "for free" because of the use of the powerful
python interpreter.

Using python also significantly reduces the development effort for HOOMD itself because
no scripting language needs to be written. Instead, low level Compute, Analyzer, etc...
classes only need be exposed using boost::python. The actual "scripting" commands
are then just python commands that use these low level classes.

\section sec_hoomd_script_requirements Requirements
 -# Script commands will all be python classes, to keep some amount of sanity when programming
 -# Hide this object oriented nature as much as possible and present the user with what
 	appears to be a simple procedural script
 -# Mimic LAMMPS commands wherever possible
 -# Support restart files .... somehow
 -# For progress reporting, each command called should print out (to the best
 of its ability) what the user called script command was. This facilitates
 debugging if there is a crash somewhere in the script.

Note: current thoughts on the method by which restart files will be supported are fuzzy.
The plan is to identify every compute/analyzer/updater etc... by an autogenerated name
when added to the System. Then the user script can request it by name after loading the
restart file if needed.  A fancier version may consider saving python variables from the
__main__ namespace, but that may be fraught with peril.

\section sec_hoomd_script_implementation_notes Implementation notes

<b>0. Global variable handling</b>
Some commands will need to pass values to future commands without any user hassle. Global
variables are the only option. To implement global variables for hoomd_script, a \c globals
module will define all of them. Any other module in hoomd_script can then access any global
via \c globals.variable_name.

<b>1. Execution configuration</b>
There needs to be a way to specify the execution configuration, which includes how many CPUs/GPUs
to run on and which ones. It is probably a good idea to allow for this to be specified on the command line,
but a script command allowing potentially more customization could be helpful.

Details on this command/command line options are TBD.
The execution configuration \b must be set before any other hoomd_script commands can be run.

<b>2. ParticleData and System Initialization</b>

All Updaters, Computes and Analyzers need the ParticleData defined before their construction.
Thus, the ParticleData initialization must come before these. At the same time, the System
needs to be initialized for use by all the others. To make these available for other commands to
use in the future a global variable will be set for each of these.

Commands:<br>
\c read_xml( \a file_name )<br>
\b Arguments<br>
\a file_name: hoomd_xml file to read in<br>
\b Post: <br>
\c globals.particle_data is set to a ParticleData initialized with \a file_name<br>
\c globals.system is initialized as a System attached to the particle data<br>
\b Returns: ParticleData initialized with the data in \a file_name<br>


\c create_random( \a N, \a phi_P, \a min_dist, \a wall_offset )<br>
\b Arguments<br>
	\a N: number of particles to create<br>
	\a phi_P: volume fraction of particles in the box<br>
	\a min_dist: minimum distance apart placed particles will be<br>
	\a wall_offset: if specified (default is None), walls are created at the specified offset<br>
\b Post: <br>
\c globals.particle_data is set to a ParticleData initialized by RandomInitializer<br>
\c globals.system is initialized as a System attached to the particle data<br>
\b Returns: Initialized ParticleData

Eventually: \c create_lattice to create a system of particles on a defined lattice

It is an error to execute any initialization command more than once in a script.

<b>3. Adding Updaters/Integrators</b>

Any Updater must be created \e after an initialization command has been called. Upon creation,
the updater will read \c globals.particle_data to initialize the Updater. To facilitate
restarts, a name like \e updaterN (where N is an increasing integer) will be created
for the updater. This name will be printed to the screen for reference and
registered with the System. In case the user later wants to disable an updater from
operating, a \c disable() method will be provided which removes the updater from the System.
Another method should be provided to change the period at which the updater is called
by the system. In object-oriented fashion, these registration details will be handled by
the base class \c updater.

Each C++ Updater will have a python mirrored descendant of \c updater, which is not an
Is-A relationship. Each python updater has-A Updater that it controls. The python class
provides a user friendly system of commands for the user to create and modify the Updater.
As such, the python mirror class needs to maintain an internal reference to the C++
class it controls.

For restart purposes, updaters must take in a \a name argument in addition to any
pertinent parameters. If the updater is created with the \a name argument set,
all other parameters are ignored and the Updater with that name is extracted from
the System for use by the user.

One possible module structure is to have an "updater" module: Thus commands might
look like: sorter = updater.sorter(tau=0.5, T=1.2)

The underlying CPU or GPU c++ class should be chosen based on the execution configuration.

Commands:<br>
TODO:

One potentially confusing situation is with the specification of ForceComputes
to the Integrator. It would be too much to ask of the user to place the integrator
command after all force compute declarations. So, upon construction, the
integrator will add all (non-disabled) force computes created in the script so far.
Future force compute instantiations will need to see if an integrator has been set
and add themselves as needed.

<b>4. Adding Analyzers</b>
Analyzers will be handled in an identical way as the updaters.

<b>5. Forces</b>
Like the Updaters and Analyzers, ForceComputes will have python mirror classes.
As mentioned previously, one complication is that they will need to register themselves
with the integrator when created. But, if no integrator is defined, ForeComputes
need to add themselves to a queue (\c globals.forces) so that the Integrator
can add them when it is initialized. This list needs to be maintained even if the
Integrator is already defined so that a disabled and then recreated Integrator
can re-add all the force computes. The ForceCompute mirror class must also
include a test of whether it has been disabled.

A disable method should be provided so that
forces can be disabled if desired. Instantiation of a forcecompute also needs to
register itself with the System for later retrieval. As with the Updaters and
Analyzers, a name parameter in the constructor will result in the retrieval of
the ForceCompute from the System.

Coefficients will be defined by a dictionary. One slick advantage is that parameters
can then be identified by \e name instead of a type id number. This will be
very convenient for simulations with large numbers of types. Pair coefficients
will be specified in a \c pair_coeff class with the underlying data structure
being a dictionary of dictionaries. For example, to specify parameters for the
A-B LJ interaction: coeff[('A', 'B')] = dict(epsilon=1.1, sigma=1.0, alpha=0.5);
To make this easier on the users, a pair_coeff class will be automatically setup for them
when a pair force is created, and parameters will be set through a set method. For example:
\code
lj = pair.lj(r_cut = 3.0)
lj.pair_coeff.set('A', 'B', epsilon=1.1, sigma=1.0, alpha=0.5)
\endcode


To facilitate checking that the user has filled out all needed pairs, we need a
few things: 1) ParticleData maps types to id's and provides a list of type names.
2) \c pair_coeff provides an \c is_complete method that checks if all type
pairs are specified (counting 'A'-'B' as 'B'-'A') and that all names in the
dictionaries are OK. It should check that required values are submitted
(i.e. epsilon and sigma) and warn the user if extra values are specified (possible
typos). Optional values with default parameters should also be supported.
For convenience, a get method should be provided that tries to lookup
'A'-'B' if 'B'-'A' is not found. It should not be considered an error to specify
parameters for types that do not exist: this could allow one to write one big
force field pair_coeff (and import it) even when only running with a few types in a
particular simulations. The only requirement should be that all types
that ParticleData says exists are there.

Bond coefficients will be held by a similar data structure, but indexed
by bond name. coeff['Hbond'] = dict(k=330, r_0=0.84)

<b>6. Shared/Automatically created computes/updaters/analyzers</b>

For convenience and for the best possible matchup with LAMMPS, things
like the log writer, neighbor list, and particle sorter will need to
be created automagically by the script library. These will be stored in
the \c globals module for access by hoomd_script. But, for the most
consistent user experience with modifying parameters of these
automatically created objects, they will need to inject themselves
into main. Case in in point, setting neighbor list parameters:<br>
1) The proper OO way: globals.nlist.set_params(r_cut = 2.6)<br>
2) More user friendly way: nlist.set_params(r_cut = 2.6)<br>

The second option can work by having the creator import __main__
and set __main__.nlist = created nlist. The more user friendly way
is needed because they will be doing:<br>
pair = pair.lj(r_cut=3.0)<br>
pair.set_params(r_cut=2.6)<br>
elsewhere. The extra "globals" might confuse some users. Yes,
creating variables in the __main__ namespace is ugly and
frowned upon, but I feel it is justified.

To facilitate the capability for the user to override the
automatically created objects, they can create them. However,
this \b must be done before the object would have otherwise been
automatically created. An error should be generated if, say,
more than one neighborlist is created.

<b>7. Packages/modules</b>
The whole hoomd_script package is designed to be imported into the __main__
script as "from hoomd_script import *"

The following modules will be defined
 -# init: Methods that initialize the System
 -# globals: Home for all global variables
 -# integrate: All Integrators
 -# update: All Updaters that are not Integrators
 -# dump: Analyzers that dump configurations
 -# analyze: All other Analyzers
 -# pair: Pair potentials
 -# bond: Bond potentials
 -# force: Other misc forces
 -# coeff_manager: pair, type, and bond coefficient managers

?? Need a place for the execution configuration and log writer....
no ideas currently. There must be some other things I'm missing, too.

<b>8. Reserved words</b>
TODO: Document a list of reserved words that includes all names imported
by from hoomd_script import *, or injected into __main__ by an
automatic creation.

<b>9. System.run</b>
We can't forget the run command!

<hr>
\ref page_gpu_impl "Next Page"
*/


/*!
\page page_gpu_impl GPU implementation

<b>Rationale/Implementation notes</b>

HOOMD's design goals target the fastest possible single machine performance for MD simulations.
To obtain, this every single calculation that can be done on the GPU must be done there to maximize
performance and minimize the time spent copying data from the GPU <-> CPU. Still, all classes whether
they be GPU or CPU must interoperate with each other, so acquire methods (see \ref page_particle_data)
must be used to implement this. Every GPU compute/updater should have a corresponding CPU version,
both for comparison/validation and for use in small simulations on the CPU. Usually, this is best done
by having the GPU compute inherit from the CPU compute and override the calculation methods. In this manner
any caller of the class doesn't need to know where the calculations are to be performed.

<b>Multi-GPU</b>

To get the fastest possible single workstation performance, every GPU compute/updater should execute
on multiple GPUs. CUDA is designed for a peer to peer multi-GPU program, but that model does not
work well with HOOMD's design. So, GPUWorker was created to allow for calling GPU kernels
on multiple GPUs from a single master thread. The most common data-parallel breakdown of any
portion of MD is to assign one thread per particle. Thus, HOOMD's multi-GPU implementation
assigns N/N_gpus particles to each GPU.

Some data is stored on all GPUs (like particle positions) and others only store what is needed
on that GPU (like forces and the Neighborlist). Carefully read the documentation for these data
structures to find out which is which. Typically, a kernel will define two indices: \a idx_local is the
\b local index or the index of which particle is being handled that is local to this GPU.
\a idx_global is the global index of the particle and indexes into those data structures that
have all particles on all GPUs.

In general, all per-particle data (position, velocity, type, mass, diameter, acceleration, image, ...) will
be stored on all GPUs as these fields are often read from neighboring particles. Data structures that are computed
and used only with a single particle, like Neighbor lists, bond lists, calculated forces, and exclusion lists (to name a few),
only store the data needed on each GPU there.

Several different designs for the communication between multiple GPUs are being tested. This will be
updated with the details when a working method is found, tested, and debugged.

<b>C programming</b>

CUDA does allow some c++ features, but they are flaky and work on some platforms but not others.
As such, all CUDA code is separated into a separate cuda/ directory and into .cu files to flag that
they are to be compiled by nvcc. All code in .cu files must predominately be C code. Overall management
of the data structures and tasks is done by the mother C++ class, but the buts of the work is done by
having that class call the C procedures from the .cu files.  One area where C++ programming can be helpful
is in managing the memory allocation/deallocation in the GPU data structures. Accessor methods can even
be used on the GPU to perform some level of data-hiding, although these mask the underlying data access
pattern and may lead to confusion in how to best organize the threads and blocks. These can be done as
shown below.

\code
struct SomeDataGPU
	{
	float *data;
	__host__ cudaError_t allocate(int N);	// allocates memory in data
	__host__ cudaError_t deallocate(int N); // frees memory in data
	__device__ float accessor(int i);	// accesses element i
	}
\endcode

<b>Naming conventions</b>

Naming conventions through most of HOOMD are:
	- Name classes with mixed case, upper case for each new word : \b ThisIsAClassName
	- Member variables prefixed with \b m_ to call out that this is a member variable.
	- Member variables are all lowercase with underscores separating words: \b m_this_is_a_member_variable

To facilitate identifying variables that are on the host/device, code that has a member variable allocated on the GPU
will name it like this: \b d_this_is_a_device_member_variable (\b m_gpu_ is also an acceptable prefix). Similarly,
\b h_ will be used for host variables where there is any doubt that the particular variable might or might not be on the
device.

In NVCC code, the style is C and not C++.
	- So functions are named with a \b gpu_ prefix gpu_this_is_a_gpu_function
	- Kernels have a \b _kernel suffix : \b i_am_a_force_sum_kernel
	- Most variables and other things are named similarly.
	- \b d_ and \b h_ prefixes are to be used when appropriate
	- C argument ordering should be used where output arguments are listed first and then input arguments
	- CUDA does support pass-by-reference, it should be used whenever possible

On the C++ side, classes are usually in one file per class ClassName.cc / ClassName.h. CUDA code
in HOOMD is often tightly coupled with a given C++ class. When this is true, that CUDA code should be
in ClassName.cu / ClassName.cuh.

<b>Data structures</b>

Unfortunately, I cannot document the data structure choices at this high level. All data structures
used in the GPU computes are intimately tied to the structure of the GPU kernel. See the actual
kernel source code for more information.

<b>Dig through the code</b>

There are 1001 more nitty-gritty details that anyone implementing CUDA kernels in HOOMD must know,
and there is no way I can go over them all here at a high level. One simply must look into the existing
code and see how things are done.

<hr>
\ref page_rigid "Back to Table of Contents"

*/


/*!
\page page_rigid Rigid bodies

<b>Rationale/Implementation notes</b>

Adding rigid body dynamics to HOOMD should be fairly straightforward from a computational standpoint. Almost
everything is already almost there (as of HOOMD 0.8.0), we just need to add the data structures for storing
the rigid body data and fit the update process into the integrators. The biggest problem to overcome is basically
just the data structures. HOOMD's ParticleData setup has gotten quite bloated and is not easily extensible to
rigid bodies. A redesign is in progress that will result in the creation of a templated GPUArray class which
encapsulates the concept of a CPU data mirrored to the GPU with read/write acquire capabilities built in. Once
this array is built, it will be absolutely trivial to add new fields to ParticleData and the forthcoming rigid
data.

The fundamental definition of a rigid body (in ParticleData) is just a new integer tag added to each particle that
identifies which body the particle belongs to. Sentinel values (0? or -1 (0xffffffff)?) will be used to signify particles that
belong to no rigid body.

Executing on the GPU, this data structure is not good for the force/torque summation step. An additional data
structure must be maintained that lists the index of each particle belonging to each body. With this auxiliary
structure, a single thread can go down the list for a single body and sum the net force and torque on that body
in parallel with all the other threads doing the same calculation for different bodies.

A number of additional values will need to be stored for each rigid body. Along with the aux list of particle indices
for each body, these will all be contained in RigidData each in their own GPUArray. Since adding a new data element
will just require adding a new GPUArray member variable this can be easily done by everyone. Values are stored
in the read/write acquire capable GPUArray so that Rigid updaters / force computes / etc.. can be easily written
on the CPU and/or GPU in a mixed fashion for testing and implementation of new algorithms.

For performance reasons in simulations not using rigid bodies, the non-rigid integrators will be left as they are.
Derived classes (i.e. NVERigidUpdater) will be created that know how to ingrate mixed systems of rigid bodies
and particles.

One might imagine a simulations where forces are applied to rigid bodies as a whole (i.e. orientational forces)
or even something as crazy as "pair" potentials between pairs of rigid bodies. HOOMD's architecture should allow
for such possible future expansions. This is most naturally done by creating a RigidForceCompute class which
is managed much like the ForceCompute class so that any conceivable set of forces can be easily added. The
addition of the net force on each particle to the net force and torque on each rigid body can be done within
this framework as well, with a small bit of magic in hoomd_script to maintain the list of ForceComputes as
the source forces.

<b>Tasks</b>

Putting all this together requires the completion of a number of tasks.

 -# Create the RigidData class and integrate it with SystemData.
 -# Data initialization routines need to be written to read rigid bodies in from hoomd_xml files and initialize
 	the new RigidData.
 -# Create the RigidForceCompute base class
 -# Write the RigidNetForce class to calculate forces on rigid bodies from the net forces on particles belonging
    to that body
 -# Write derived integrators for moving the rigid bodies forward in time

 <b>Questions</b>

 Here are a few open questions at this point:
  - How is the input data specified in XML?
    - A list of the body value for each particle (ugly, as it requires the user to
    manually set the sentinel value for free particles)?
    - Or a list of bodies, with a list of indices of each particle
	belonging to that body (somewhat nicer, but now the user is able to specify one particle in two bodies requiring
	detailed error checking to prevent this)
  - Does there need to be a way to dump data like rigid body COM, orientation, etc?
  - Do pair forces need to be turned off between particles belonging to the same body?
    - Does this need to be an enableable option?

<hr>
\ref page_dev_info "Back to Table of Contents"

*/
